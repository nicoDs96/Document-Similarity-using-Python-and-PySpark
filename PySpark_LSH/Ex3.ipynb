{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"text-center\">\n",
    "<h1 class=\"display-1\" >Data Mining </h1>\n",
    "<h3> Homework 2</h3>\n",
    "<h3> Di Santo Nicola - 1853049</h3>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EX. 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities From Ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, hashlib, math, time\n",
    "from random import randint, seed\n",
    "seed(16)\n",
    "\n",
    "\n",
    "class hashFamily:\n",
    "    def __init__(self, i):\n",
    "        self.resultSize = 8 # how many bytes we want back\n",
    "        self.maxLen = 20 # how long can our i be (in decimal)\n",
    "        self.salt = str(i).zfill(self.maxLen)[-self.maxLen:]\n",
    "        self.id = i\n",
    "        \n",
    "    def get_hash_value(self, el_to_hash):\n",
    "        return int(hashlib.sha1(str(el_to_hash).encode('utf-8') + self.salt.encode('utf-8')).hexdigest()[-self.resultSize:], 16)\n",
    "    \n",
    "\n",
    "class shingler:\n",
    "    def __init__(self, k):\n",
    "        \n",
    "        if k > 0:\n",
    "            self.k = int(k)\n",
    "        else:\n",
    "            self.k = 10\n",
    "        \n",
    "    #inner class utility\n",
    "    def process_doc(self, document):\n",
    "        return re.sub(\"( )+|(\\n)+\",\" \",document).lower()\n",
    "    \n",
    "    def get_shingles(self, document):\n",
    "        shingles = set()\n",
    "        document= self.process_doc(document)\n",
    "        for i in range(0, len(document)-self.k+1 ):\n",
    "            shingles.add(document[i:i+self.k])\n",
    "        return shingles\n",
    "    \n",
    "    def get_k(self):\n",
    "        return self.k\n",
    "    \n",
    "    #return sorted hash\n",
    "    def get_hashed_shingles(self, shingles_set):\n",
    "        hash_function = hashFamily(0)\n",
    "        return sorted( {hash_function.get_hash_value(s) for s in shingles_set} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------------+-------------------+--------------------+\n",
      "|               Title|   Short Description|            Location|      Price (Euro)|          Timestamp|             Url Adv|\n",
      "+--------------------+--------------------+--------------------+------------------+-------------------+--------------------+\n",
      "|Studio accessoria...|Affitto studio a ...|               Roma |              450 | 12 ottobre, 11:32 |https://www.kijij...|\n",
      "|Negozio 169Mq per...|Privato affitta n...|Prenestino / Casi...|            1.700 | 12 ottobre, 08:45 |https://www.kijij...|\n",
      "|Negozio in tiburt...|Negozio c/1 roma ...|Tiburtino / Colla...|            6.000 | 17 October, 21:20 |https://www.kijij...|\n",
      "|Studio medico via...|Studio medico avv...|Trieste / Somalia...|              200 | 17 October, 20:22 |https://www.kijij...|\n",
      "|Cerco: Appartamen...|Donna lavoratrice...|               Roma |Contatta l'utente | 17 October, 19:39 |https://www.kijij...|\n",
      "|Elegante studio m...|Studio medico con...|Flaminio / Pariol...|Contatta l'utente | 17 October, 19:31 |https://www.kijij...|\n",
      "|Ufficio su strada...|A pochi metri da ...|San Giovanni / Ap...|              500 | 17 October, 19:14 |https://www.kijij...|\n",
      "|  Camera in affitto |Camera per studen...|Prenestino / Casi...|              350 | 17 October, 18:53 |https://www.kijij...|\n",
      "|Magazzino Via Giu...|Affittasi Locale ...|               Roma |            1.400 | 17 October, 18:08 |https://www.kijij...|\n",
      "|Negozio + Piazzal...|Privato AFFITTA S...|               Roma |Contatta l'utente | 17 October, 18:06 |https://www.kijij...|\n",
      "|Locale uso studio...|Affittasi N.1 loc...|Trieste / Somalia...|              580 | 17 October, 17:34 |https://www.kijij...|\n",
      "|Sala per Feste pe...|Spazio modulabile...|Tuscolano / Don B...|              400 | 17 October, 16:57 |https://www.kijij...|\n",
      "|Sala per feste a ...|Affitto locale a ...|Tuscolano / Don B...|              350 | 17 October, 16:57 |https://www.kijij...|\n",
      "|  Box insonorizzato |Box con soppalco ...|Tiburtino / Colla...|Contatta l'utente | 17 October, 16:30 |https://www.kijij...|\n",
      "|Stanze per Medici...|Affittasi 1 o 2 S...|Nomentano / Bologna |              120 | 17 October, 16:01 |https://www.kijij...|\n",
      "|Appartamento in m...|Appartamento in v...|        ~Altre zone |Contatta l'utente | 17 October, 15:50 |https://www.kijij...|\n",
      "|Location per fest...|Per una festa di ...|               Roma |Contatta l'utente | 17 October, 14:30 |https://www.kijij...|\n",
      "|Locale commercial...|Affitto locale co...|               Roma |              450 | 17 October, 14:26 |https://www.kijij...|\n",
      "|Loft arredato zon...|Appartamento nuov...|               Roma |              650 | 17 October, 14:24 |https://www.kijij...|\n",
      "|Appartamento arre...|Appartamento nuov...|               Roma |              650 | 17 October, 13:14 |https://www.kijij...|\n",
      "+--------------------+--------------------+--------------------+------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "file_path = \"../Ex2/dataset_rent_rome_kijiji.tsv\"  \n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "segments = spark.read.csv( path=file_path, header=True, sep='\\t' )\n",
    "\n",
    "#segments.persist() # to avoid lazy behaviour and store dataset in memory\n",
    "segments.show() # data preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               Title|   Short Description|doc_id|\n",
      "+--------------------+--------------------+------+\n",
      "|Studio accessoria...|Affitto studio a ...|     0|\n",
      "|Negozio 169Mq per...|Privato affitta n...|     1|\n",
      "|Negozio in tiburt...|Negozio c/1 roma ...|     2|\n",
      "|Studio medico via...|Studio medico avv...|     3|\n",
      "|Cerco: Appartamen...|Donna lavoratrice...|     4|\n",
      "|Elegante studio m...|Studio medico con...|     5|\n",
      "|Ufficio su strada...|A pochi metri da ...|     6|\n",
      "|  Camera in affitto |Camera per studen...|     7|\n",
      "|Magazzino Via Giu...|Affittasi Locale ...|     8|\n",
      "|Negozio + Piazzal...|Privato AFFITTA S...|     9|\n",
      "|Locale uso studio...|Affittasi N.1 loc...|    10|\n",
      "|Sala per Feste pe...|Spazio modulabile...|    11|\n",
      "|Sala per feste a ...|Affitto locale a ...|    12|\n",
      "|  Box insonorizzato |Box con soppalco ...|    13|\n",
      "|Stanze per Medici...|Affittasi 1 o 2 S...|    14|\n",
      "|Appartamento in m...|Appartamento in v...|    15|\n",
      "|Location per fest...|Per una festa di ...|    16|\n",
      "|Locale commercial...|Affitto locale co...|    17|\n",
      "|Loft arredato zon...|Appartamento nuov...|    18|\n",
      "|Appartamento arre...|Appartamento nuov...|    19|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Title: string, Short Description: string, doc_id: bigint]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "dataset = segments.select('Title','Short Description').withColumn(\"doc_id\", monotonically_increasing_id())\n",
    "dataset.show()\n",
    "dataset.persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shingling\n",
    "<p>For shinglings we cannot parallelize on a single document but among all the documents of the dataset. The Map function for shingling will then only distribute documents among worker nodes in the network producing (doc id,Shingles set, hash_id) pairs. The reduce function is useless for now. This step can be thought as a simple transformation step on the Spark dataframe, in this way we are chunking the characteristc matrix by column.</p> <p>Since we are not on a cluster but on a single local machine, the result will be the same as using the shingler class only, however I slightly modify the algorithm used in Ex2 to use map-reduce paradigm.</p>\n",
    "<p>The hash_id value is used to simplify the minwise hashing map step below: the hash function to use is already encoded into distributed data and next map step will be easier. With this approach we pay in terms of pair generated as output. However, since the dataset size is small it is still possible to consider this approach.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingling_map(row):\n",
    "    out = [] \n",
    "    sh_instance = shingler(10)\n",
    "    hashed_shingles = sh_instance.get_hashed_shingles( sh_instance.get_shingles( row['Title']+\" \"+row['Short Description'] ) ) \n",
    "    signature_size = 50\n",
    "    for i in range(0,signature_size): #signature size\n",
    "        out.append( (row['doc_id'], hashed_shingles, i ) ) \n",
    "    #return an iterator to use flatMap => produce more than one key-value pair as output (namely one per hash function)\n",
    "    return iter(out)\n",
    "\n",
    "    \n",
    "#Use rdd.collect() to get all data from workers to driver. In the specific case it returns a list of [(doc_id, shingle_set),...] where shingle_set = [sh1,sh2,...sh_n]\n",
    "docId_shingleset_hfunc = dataset.rdd.flatMap(shingling_map)\n",
    "#doc_id_shingle_set_pair.toDF().show() to see results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minwise Hashing\n",
    "For minhashing, since the characteristic matrix is chunked by column, we can easly design a map-reduce task to produce the signature matrix: \n",
    "<ul>\n",
    "    <li>Map: take as input $(doc\\_id, shingle\\_set,h_i)$ tuple with $h_i$ an hash function from hash family defined above and produce the minhash value of the set for that given hash function: $(doc\\_id,  min\\_hash)$</li>\n",
    "    <li>Reduce: the reduce step is simplified to a simple, built in, <i>groupByKey()</i> action and returns a column of the signature matrix: $(doc\\_id, minhash\\_signature)$</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minhash_map(docId_ShingleSet_hFunct):\n",
    "    doc_id = docId_ShingleSet_hFunct[0]\n",
    "    shingles = docId_ShingleSet_hFunct[1]\n",
    "    hash_f = hashFamily( docId_ShingleSet_hFunct[2] )\n",
    "    min_h = math.inf\n",
    "    for el in shingles:\n",
    "        hash_value = hash_f.get_hash_value(el)\n",
    "        if hash_value < min_h:\n",
    "            min_h = hash_value\n",
    "            \n",
    "    return (doc_id, min_h )\n",
    "\n",
    "\n",
    "# as Reduce step we use the built in groupByKey() since no extra operation is needed.\n",
    "sig_matrix = docId_shingleset_hfunc.map(minhash_map).groupByKey().map(lambda x : (x[0], list(x[1])))\n",
    "sig_matrix.persist() # to do not compute it every time since its an expensive computation\n",
    "#examples of content are:\n",
    "dummy = ''' \n",
    "\n",
    "sig_matrix.toDF().show()\n",
    "+---+--------------------+\n",
    "| _1|                  _2|\n",
    "+---+--------------------+\n",
    "|  0|[4781236, 3571529...|\n",
    "|  1|[2130985, 1259985...|\n",
    "|  2|[40365747, 857165...|\n",
    "|  3|[45686806, 309643...|\n",
    "|  4|[8369628, 2430029...|\n",
    "|  5|[39442189, 141331...|\n",
    "|  6|[2999960, 2797603...|\n",
    "|  7|[30153137, 360865...|\n",
    "|  8|[49068494, 273188...|\n",
    "|  9|[1344538, 5459810...|\n",
    "| 10|[7879527, 7920209...|\n",
    "| 11|[7700705, 6769721...|\n",
    "| 12|[272635, 1963659,...|\n",
    "| 13|[2999960, 1486707...|\n",
    "| 14|[1171357, 5761465...|\n",
    "| 15|[21922776, 310437...|\n",
    "| 16|[1813937, 5020567...|\n",
    "| 17|[76445429, 575841...|\n",
    "| 18|[3295091, 2904792...|\n",
    "| 19|[3295091, 2904792...|\n",
    "+---+--------------------+\n",
    "\n",
    "\n",
    "\n",
    "sig_matrix.take(1)\n",
    "[(0,\n",
    "  [4781236,\n",
    "   35715291,\n",
    "   567066,\n",
    "   390697,\n",
    "   5723987,\n",
    "   2079438,\n",
    "   6462491,\n",
    "   7336631,\n",
    "   1957111,\n",
    "   39763103])]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality Sensitive Hashing\n",
    "<p>In this step we are going to: \n",
    "<ol>\n",
    "    <li>Divide Signature Matrix into bands</li>\n",
    "    <li>Hash the bands to get candidates pairs</li>\n",
    "    <li>Check if candidates are actually similar</li>\n",
    "</ol>\n",
    "The first two step are computed among worker nodes while the last one, as well as bruteforce comparisons, is computed on master node since it requires lookup to signature martix. Lookup are not allowed on rdd datastructure from worker nodes.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IMPLEMENTING POINT 1. AND 2. ABOVE WITH A SINGLE MAP-REDUCE STEP\n",
    "# with this function we split signature into equal subset, assign this subset to a band, hash the subset to get its bucket nr.\n",
    "# the return value will be a key-value pair with key = (band_id, bucket) and value = doc_id\n",
    "# Note: The key include the band-id because we used a single hash function to produce buckets for different bands \n",
    "#       and its fine as long as we produce different buckets for the bands (from chap 3.4 of the book)\n",
    "\n",
    "# The reduce step will then be a simple groupByKey() and will give us key = (band_id, bucket), value = list of candidates\n",
    "# Note: an additional map is performed to transform iterable object to list \n",
    "def map_buckets(row):\n",
    "    \n",
    "    band_nr = 5\n",
    "    row_nr = 10\n",
    "    doc_id = row[0]\n",
    "    doc_sign = row[1]\n",
    "    hash_funct = hashFamily(1)\n",
    "    out = []\n",
    "    \n",
    "    for i in range(0,band_nr):\n",
    "        band_id = i\n",
    "        idx = i*row_nr   \n",
    "        set_col = ' '.join(str(x) for x in doc_sign[idx:idx+row_nr])\n",
    "        bucket = hash_funct.get_hash_value(set_col)\n",
    "        out.append( ( (band_id, bucket), doc_id)  )\n",
    "    \n",
    "    return iter(out) #since we are going to return multiple tuple we call flatMap() and return an iterator on those tuple\n",
    "    \n",
    "candidates = sig_matrix.flatMap(map_buckets).groupByKey().map(lambda x : (x[0], list(x[1]))) #the map() is used to convert iterator to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of data obtained from the above map-reduce-map step\n",
    "dummy_candidates_ex = ''' candidates.take(1)\n",
    "\n",
    "[((0, 2699225922),\n",
    "  [0,\n",
    "   22,\n",
    "   44,\n",
    "   66,\n",
    "   88,\n",
    "   154,\n",
    "   176,\n",
    "   220,\n",
    "   330,\n",
    "   352,\n",
    "   440,\n",
    "   603,\n",
    "   647,\n",
    "   669,\n",
    "   735,\n",
    "   757,\n",
    "   779,\n",
    "   801,\n",
    "   867,\n",
    "   889,\n",
    "   933,\n",
    "   1043,\n",
    "   1087,\n",
    "   1109,\n",
    "   1153,\n",
    "   1219,\n",
    "   1241,\n",
    "   1263,\n",
    "   1373,\n",
    "   1417,\n",
    "   1461,\n",
    "   1505,\n",
    "   1615,\n",
    "   1637,\n",
    "   1703,\n",
    "   1725,\n",
    "   1813,\n",
    "   1857,\n",
    "   1901,\n",
    "   1967,\n",
    "   1989,\n",
    "   2011,\n",
    "   2033,\n",
    "   2077,\n",
    "   2099,\n",
    "   2143,\n",
    "   2187,\n",
    "   2297,\n",
    "   2319,\n",
    "   2363,\n",
    "   2451,\n",
    "   2517,\n",
    "   2561])]\n",
    "   \n",
    "   candidates.toDF().show()\n",
    "    +---------------+--------------------+\n",
    "|             _1|                  _2|\n",
    "+---------------+--------------------+\n",
    "|[0, 2699225922]|[0, 22, 44, 66, 8...|\n",
    "| [1, 773591669]|[0, 22, 44, 66, 8...|\n",
    "|[2, 3964590397]|[0, 22, 44, 66, 8...|\n",
    "|[3, 2852783413]|[0, 22, 44, 66, 8...|\n",
    "|[4, 1818672075]|[0, 22, 44, 66, 8...|\n",
    "|[0, 1417325792]|[1, 23, 45, 67, 8...|\n",
    "| [1, 485583475]|[1, 23, 45, 67, 8...|\n",
    "| [2, 823708336]|[1, 23, 45, 67, 8...|\n",
    "|[3, 3657351485]|[1, 23, 45, 67, 8...|\n",
    "|[4, 1476921203]|[1, 23, 45, 67, 8...|\n",
    "|[0, 1902259772]|           [2, 1257]|\n",
    "| [1, 869183940]|                 [2]|\n",
    "|[2, 3109000709]|           [2, 1257]|\n",
    "| [3, 319528939]|                 [2]|\n",
    "|[4, 2004917486]|                 [2]|\n",
    "|[0, 2096920849]|                 [3]|\n",
    "|[1, 2844631318]|                 [3]|\n",
    "|[2, 3397623607]|                 [3]|\n",
    "|[3, 1948736288]|                 [3]|\n",
    "|[4, 3121195643]|                 [3]|\n",
    "+---------------+--------------------+\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND 10322 SIMILAR PAIRS\n"
     ]
    }
   ],
   "source": [
    "## This part of the algorithm must be performed at driver since lookup into signature matrix are required and \n",
    "## spark does not allow lookup from worker nodes. \n",
    "\n",
    "similar_pairs = set()\n",
    "sig_df = sig_matrix\n",
    "sig_df.persist() #store it in memory to avoid lazy behaviour\n",
    "sig_df = sig_df.collect() #collect all the distributed record to speedup computation\n",
    "start_time = time.time()\n",
    "\n",
    "for candidates_list in candidates.map(lambda x: x[1]).collect(): #collecting all candidate lists from worker nodes\n",
    "    \n",
    "    candidates_nr = len(candidates_list)\n",
    "    \n",
    "    for i in range(0,candidates_nr-1):\n",
    "        for j in range(i+1,candidates_nr):\n",
    "            \n",
    "            doc_id_1 = candidates_list[i]\n",
    "            doc_id_2 = candidates_list[j]\n",
    "            \n",
    "            docId_Sig_1 = sig_df[doc_id_1]\n",
    "            docId_Sig_2 = sig_df[doc_id_2]\n",
    "            if docId_Sig_1[0] != doc_id_1:\n",
    "                raise Exception(\"DocId is %d while retrived %d from signature matrix.\"%(doc_id_1,docId_Sig_1[0]))\n",
    "            if docId_Sig_2[0] != doc_id_2:\n",
    "                raise Exception(\"DocId is %d while retrived %d from signature matrix.\"%(doc_id_1,docId_Sig_2[0])) \n",
    "                \n",
    "            sig_1 = set(docId_Sig_1[1]) #transform signature into a set\n",
    "            sig_2 = set(docId_Sig_2[1])\n",
    "            #sig_1 = sig_df.filter(\"_1 ==\"+\" \"+ str(doc_id_1) ).select('_2') #get signature of doc 1\n",
    "            #sig_2 = sig_df.filter(\"_1 ==\"+\" \"+ str(doc_id_2) ).select('_2') #get signature of doc 2\n",
    "            \n",
    "            #sig_2 = set(sig_2.collect()[0]['_2'])\n",
    "            js = len(sig_1.intersection(sig_2) ) / len(sig_1.union(sig_2) ) #Compute Jaccard'Similarity\n",
    "            if js >= 0.8:\n",
    "                pair = tuple(sorted((doc_id_1,doc_id_2) ))\n",
    "                similar_pairs.add(  pair   ) \n",
    "\n",
    "end_time = time.time()  \n",
    "\n",
    "lsh_time = end_time - start_time\n",
    "\n",
    "print(\"FOUND %d SIMILAR PAIRS\" %len(similar_pairs))\n",
    "\n",
    "#sim_pairs = candidates.flatMap(get_similar_items).groupByKey().map(lambda x : (x[0], set(serted(x[1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bruteforce search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND 12986 SIMILAR PAIRS WITH BRUTEFORCE\n"
     ]
    }
   ],
   "source": [
    "DocID_Shingles=docId_shingleset_hfunc.map(lambda x: (x[0],x[1]) ).reduceByKey(lambda x,y: x).collect()\n",
    "#DocID_Shingles.persist()\n",
    "doc_nr = len(DocID_Shingles)\n",
    "similar_pairs_bf = set()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0,doc_nr-1):\n",
    "    for j in range(i,doc_nr):\n",
    "        doc_id_1 = DocID_Shingles[i][0] #DocID_Shingles[i] is  tuple (doc_id, shingle_set)\n",
    "        doc_id_2 = DocID_Shingles[j][0]\n",
    "        \n",
    "        sig_1 = set(DocID_Shingles[i][1]) #transform shingle_set into a python set data structure\n",
    "        sig_2 = set(DocID_Shingles[j][1])\n",
    "        js = len(sig_1.intersection(sig_2) ) / len(sig_1.union(sig_2) ) #Compute Jaccard'Similarity\n",
    "        if js >= 0.8:\n",
    "            pair = tuple(sorted((doc_id_1,doc_id_2) ))\n",
    "            similar_pairs_bf.add(  pair   ) \n",
    "            \n",
    "end_time = time.time()\n",
    "    \n",
    "bf_time = end_time - start_time \n",
    "\n",
    "print(\"FOUND %d SIMILAR PAIRS WITH BRUTEFORCE\" %len(similar_pairs_bf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXECUTION REPORT\n",
      "LSH\n",
      "10319\tSIMILAR ITEMS\n",
      "1.24\tSECONDS\n",
      "\n",
      "BRUTEFORCE\n",
      "12986\tSIMILAR ITEMS\n",
      "336.77\tSECONDS\n",
      "\n",
      "10318 SIMILAR PAIR DISCOVERED WITH BRUTEFORCE AND LSH\n",
      "\n",
      "1 NUMBER OF FALSE POSITIVE SIMILARITIES WITH LSH\n",
      "\n",
      "2668 NON DETECTED SIMILARITIES BY LSH\n"
     ]
    }
   ],
   "source": [
    "print(\"EXECUTION REPORT\")\n",
    "print(\"LSH\\n%d\\tSIMILAR ITEMS\\n%.2f\\tSECONDS\\n\"%(len(similar_pairs), lsh_time))\n",
    "print(\"BRUTEFORCE\\n%d\\tSIMILAR ITEMS\\n%.2f\\tSECONDS\\n\"%(len(similar_pairs_bf), bf_time))\n",
    "print(\"%d SIMILAR PAIR DISCOVERED WITH BRUTEFORCE AND LSH\\n\"%( len(similar_pairs.intersection(similar_pairs_bf)) ))\n",
    "\n",
    "print(\"%d NUMBER OF FALSE POSITIVE SIMILARITIES WITH LSH\\n\"%(len(similar_pairs) - len(similar_pairs.intersection(similar_pairs_bf)) ) )\n",
    "\n",
    "print(\"%d NON DETECTED SIMILARITIES BY LSH\"%(len(similar_pairs_bf) - len(similar_pairs.intersection(similar_pairs_bf))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "From LSH similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: 0\n",
      "+--------------------+--------------------+------+\n",
      "|               Title|   Short Description|doc_id|\n",
      "+--------------------+--------------------+------+\n",
      "|Avviato studio ps...|Avviato studio di...|   506|\n",
      "+--------------------+--------------------+------+\n",
      "\n",
      "Similar Doc:\n",
      "+--------------------+--------------------+------+\n",
      "|               Title|   Short Description|doc_id|\n",
      "+--------------------+--------------------+------+\n",
      "|Avviato studio ps...|Avviato studio di...|   568|\n",
      "+--------------------+--------------------+------+\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: 1\n",
      "+--------------------+--------------------+------+\n",
      "|               Title|   Short Description|doc_id|\n",
      "+--------------------+--------------------+------+\n",
      "|Negozio 169Mq per...|Privato affitta n...|    89|\n",
      "+--------------------+--------------------+------+\n",
      "\n",
      "Similar Doc:\n",
      "+--------------------+--------------------+------+\n",
      "|               Title|   Short Description|doc_id|\n",
      "+--------------------+--------------------+------+\n",
      "|Negozio 169Mq per...|Privato affitta n...|  1616|\n",
      "+--------------------+--------------------+------+\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Doc: 2\n",
      "+--------------------+--------------------+------+\n",
      "|               Title|   Short Description|doc_id|\n",
      "+--------------------+--------------------+------+\n",
      "|Negozio 169Mq per...|Privato affitta n...|  1154|\n",
      "+--------------------+--------------------+------+\n",
      "\n",
      "Similar Doc:\n",
      "+--------------------+--------------------+------+\n",
      "|               Title|   Short Description|doc_id|\n",
      "+--------------------+--------------------+------+\n",
      "|Negozio 169Mq per...|Privato affitta n...|  1990|\n",
      "+--------------------+--------------------+------+\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,3):\n",
    "    pair = similar_pairs.pop()\n",
    "    doc1 = pair[0]\n",
    "    doc2 = pair[1]\n",
    "    print(\"Doc: \"+str(i))\n",
    "    dataset.where('doc_id == '+str(doc1)).show()\n",
    "    print(\"Similar Doc:\")\n",
    "    dataset.where('doc_id == '+str(doc2)).show()\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
